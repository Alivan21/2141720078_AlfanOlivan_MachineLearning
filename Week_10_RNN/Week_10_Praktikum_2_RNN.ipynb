{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Praktikum 10_2 - Recurrent Neural Network (RNN)**"
      ],
      "metadata": {
        "id": "QX3KRtkeW17f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Alfan Olivan\n",
        "\n",
        "NIM : 2141720078"
      ],
      "metadata": {
        "id": "GzSiX2s5XIbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Praktikum 2**"
      ],
      "metadata": {
        "id": "OCL8-AY1XMyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**"
      ],
      "metadata": {
        "id": "_hhp75vDXi9W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7lECwRh-WoD7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wKNvOnjXkhK",
        "outputId": "9b487596-83e7-4afe-80ec-f7b5551e782a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "78K0hiZgYTdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5LJYLpfYOwA",
        "outputId": "57466b9a-df89-46aa-f6e6-5c2986a20b1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmFsrWfTYWSv",
        "outputId": "331807ef-b1fd-4042-b072-d3d136e5be60"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyrwUyFnYaNj",
        "outputId": "82b8b223-a194-44f5-c7bc-43524356ffd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Olah teks**"
      ],
      "metadata": {
        "id": "Eut4CbQaYh-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  * Vectorize Teks"
      ],
      "metadata": {
        "id": "4v0ZfA5HZXST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars=tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wh2VJzSYe-O",
        "outputId": "22f3b40b-344a-4f1d-9cb6-faf48d2d2c09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab),mask_token=None)"
      ],
      "metadata": {
        "id": "GedxiLALZpLC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scmTtsRZZsfF",
        "outputId": "48fb4c24-708c-4d23-f87e-d6465a967ca1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "I_JdhfLUaXAD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDlB_fzgaZWC",
        "outputId": "634763f3-2a53-486e-da89-d880d743bc45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_u2EVldZyVu",
        "outputId": "5f80f671-6895-45f5-b96d-f610a7c7b2ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "gv3lxLKhZzxn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Prediksi"
      ],
      "metadata": {
        "id": "4GNYOZrYZ802"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TosGZXxBZ7jd",
        "outputId": "97547a85-955d-406b-f1fb-3135a2ffeb49"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "kUqHDVS-aFVB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGJgHAUBaKJ8",
        "outputId": "7c2b40a7-1d7e-4842-e68e-419a3b873200"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=100"
      ],
      "metadata": {
        "id": "lHzUVDunagqZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pe2Lq3aIat71",
        "outputId": "fec68bb7-e866-4a09-ea8f-ec62eb35298e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63gEJdFtawq1",
        "outputId": "82fed00b-be5c-419f-b570-5bb75b40f747"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "IoNnaMeibjcH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1NnSo7mbuqK",
        "outputId": "72dfe102-c9fd-4694-d2ff-e71927963925"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "1vcEpN3mbwpv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\",text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\",text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOzAg0yubzjr",
        "outputId": "e346f134-b951-40c7-88e7-e9848133edfd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Membuat Batch Training"
      ],
      "metadata": {
        "id": "e5eaJJ8-ejgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OL51LsEfelaY",
        "outputId": "abbee8ce-5f8d-40ba-cccf-cedcae039465"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Membuat Model**"
      ],
      "metadata": {
        "id": "j25f9hm_eouj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "ZXOQCguJeqo_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "DlQHHmE2euGV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "hVeYxcizevr6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uji Model**"
      ],
      "metadata": {
        "id": "LmekyvUke1L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2mJE3-Ge2eg",
        "outputId": "3ecef4cd-32c7-4c6c-a51f-af0310273fef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl5YCSC-e4eY",
        "outputId": "b73185f1-d810-43b9-c4b1-1822dc31312f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices=tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices=tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "metadata": {
        "id": "vM5eEU5Ue6JQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orffKFxrfACM",
        "outputId": "fa10c137-f96a-4734-e8b5-6c2accca963b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22,  1, 50,  1, 44,  0,  0, 58, 49, 24, 35, 53, 10,  2, 41, 23, 27,\n",
              "        7, 13, 14, 19, 46, 39,  3,  2, 13, 63, 10, 62, 25, 12, 13, 54,  3,\n",
              "        5, 23, 28,  9, 57, 41, 15, 22, 43, 51, 65, 61, 50, 59, 62, 47, 64,\n",
              "       17, 44, 57, 53, 48, 43, 53, 52, 26, 43, 18,  6, 32, 56, 21, 52, 48,\n",
              "       58, 58, 12, 55, 31, 23, 38, 27, 60, 18, 51, 25, 62, 39,  7, 32, 17,\n",
              "       36, 30, 52, 26, 60, 30,  2, 22, 56, 37, 18, 57, 56, 17, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\",text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\",text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYRACWT-e_4I",
        "outputId": "71e1f771-fa09-4f61-fe7c-6e9c2744a20d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\" the gods had nothing else to do\\nBut to confirm my curses! Could I meet 'em\\nBut once a-day, it would\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"I\\nk\\ne[UNK][UNK]sjKVn3 bJN,?AFgZ! ?x3wL;?o!&JO.rbBIdlzvktwhyDernidnmMdE'SqHmiss;pRJYNuElLwZ,SDWQmMuQ IqXErqDK\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "id": "QAkUSB5DfFqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "KZGTQyXlfHOU"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8_F0Xe6fJu8",
        "outputId": "dd948be0-e062-4588-d8d2-fb4552010f24"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1899014, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q-FiYUVfMI5",
        "outputId": "4a4e2218-ff68-461a-944f-c8922fa7a801"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.01628"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "tDn3VTY-fOR0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "ILcCOcrDfS6u"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS=20\n",
        "history=model.fit(dataset,epochs=EPOCHS,callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKVqJTnxfVTD",
        "outputId": "612d31d8-9d13-4ae3-df44-cdd3a84dc35a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 53ms/step - loss: 2.7165\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.9819\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.7043\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 14s 55ms/step - loss: 1.5436\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.4457\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3778\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.3266\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.2819\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 14s 58ms/step - loss: 1.2403\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.2005\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.1609\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.1195\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0762\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.0299\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.9820\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.9319\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 0.8786\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8266\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7759\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Teks**"
      ],
      "metadata": {
        "id": "Z1pVLj_ag3Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "Odj-w9oxg75Y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model=OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "vQnTL6AOg-gk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olCkRxlMhDKb",
        "outputId": "b7902753-16f0-4b80-a582-75b93831544e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "If he holds here at thy father's brows\n",
            "Affection, for my pardon, I will plot emir morn.\n",
            "Your thief thinkless, uncle may; served it so man\n",
            "And a desire of it our Bohemia\n",
            "\n",
            "FLORIZEL:\n",
            "O sea!\n",
            "Why most you been proved a friend of little.\n",
            "\n",
            "PAULINA:\n",
            "I do beseech you.\n",
            "\n",
            "CORIOLANUS:\n",
            "No more.\n",
            "\n",
            "First Senator:\n",
            "No, if I know 'tis man's his brother's life!\n",
            "\n",
            "All:\n",
            "An if he marr'd\n",
            "The open air, which plead'st thou owest no wife.\n",
            "\n",
            "BAPTISTA:\n",
            "How aged as toucheth use to fear, banisher,\n",
            "Our commission noteight it,\n",
            "Or my husband methinks From those thoughts\n",
            "At Oxffran faults,--O him\n",
            "Romeo is banish'd, shall be bold and madam;\n",
            "Your modest sight-owest spend as O, that nuese\n",
            "If Norfolk sens I went advantage thy strong-women!\n",
            "\n",
            "Youngs:\n",
            "Wealth! We have been thought on Hereford's sink,\n",
            "Lovel about me on his short brave weighter,\n",
            "Unward'st thou art up forth against their love\n",
            "Be sadish'd if thou couldst, and try it is your king:\n",
            "Beseech your\n",
            "cabin: I do feel the wisdom of\n",
            "Fromens their speedings, and holy subject hi \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.9023938179016113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ekspor Model Generator**"
      ],
      "metadata": {
        "id": "P0uBYriOhPbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_FxH9w-hLMO",
        "outputId": "604ba2a0-bb3b-413e-8adb-5498e3d69eae"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x79ecfc623fd0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "I_DeY1TnhW92",
        "outputId": "51685537-289b-4922-ac08-3ff3144a9304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Good madam; then I were well extrewite thee\n",
            "Past the king, who have not wined ere thou hast\n",
            "further\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return {'loss': loss}"
      ],
      "metadata": {
        "id": "i1duvm2-AZUd"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "BxGE7YbXAuXF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "oc0MQ7hxAyHd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaiO_rbvA0-1",
        "outputId": "87be9d5a-3a86-4d5a-9e6a-4fb16f69bfbf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 15s 60ms/step - loss: 2.7217\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x79ed35bcce20>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-S977mHA5YM",
        "outputId": "9d041576-984e-4663-a604-eccc20512049"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1880\n",
            "Epoch 1 Batch 50 Loss 2.0728\n",
            "Epoch 1 Batch 100 Loss 1.9441\n",
            "Epoch 1 Batch 150 Loss 1.8343\n",
            "\n",
            "Epoch 1 Loss: 1.9970\n",
            "Time taken for 1 epoch 14.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8294\n",
            "Epoch 2 Batch 50 Loss 1.7718\n",
            "Epoch 2 Batch 100 Loss 1.7112\n",
            "Epoch 2 Batch 150 Loss 1.6446\n",
            "\n",
            "Epoch 2 Loss: 1.7150\n",
            "Time taken for 1 epoch 11.21 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5816\n",
            "Epoch 3 Batch 50 Loss 1.5399\n",
            "Epoch 3 Batch 100 Loss 1.6298\n",
            "Epoch 3 Batch 150 Loss 1.4969\n",
            "\n",
            "Epoch 3 Loss: 1.5506\n",
            "Time taken for 1 epoch 11.56 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4614\n",
            "Epoch 4 Batch 50 Loss 1.4558\n",
            "Epoch 4 Batch 100 Loss 1.4466\n",
            "Epoch 4 Batch 150 Loss 1.4692\n",
            "\n",
            "Epoch 4 Loss: 1.4496\n",
            "Time taken for 1 epoch 11.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.4087\n",
            "Epoch 5 Batch 50 Loss 1.3667\n",
            "Epoch 5 Batch 100 Loss 1.3754\n",
            "Epoch 5 Batch 150 Loss 1.4103\n",
            "\n",
            "Epoch 5 Loss: 1.3805\n",
            "Time taken for 1 epoch 12.17 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3098\n",
            "Epoch 6 Batch 50 Loss 1.3183\n",
            "Epoch 6 Batch 100 Loss 1.3861\n",
            "Epoch 6 Batch 150 Loss 1.3384\n",
            "\n",
            "Epoch 6 Loss: 1.3275\n",
            "Time taken for 1 epoch 12.04 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2712\n",
            "Epoch 7 Batch 50 Loss 1.2361\n",
            "Epoch 7 Batch 100 Loss 1.3036\n",
            "Epoch 7 Batch 150 Loss 1.3009\n",
            "\n",
            "Epoch 7 Loss: 1.2826\n",
            "Time taken for 1 epoch 12.07 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2358\n",
            "Epoch 8 Batch 50 Loss 1.2374\n",
            "Epoch 8 Batch 100 Loss 1.2626\n",
            "Epoch 8 Batch 150 Loss 1.2488\n",
            "\n",
            "Epoch 8 Loss: 1.2415\n",
            "Time taken for 1 epoch 11.78 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1843\n",
            "Epoch 9 Batch 50 Loss 1.1807\n",
            "Epoch 9 Batch 100 Loss 1.1956\n",
            "Epoch 9 Batch 150 Loss 1.2017\n",
            "\n",
            "Epoch 9 Loss: 1.2014\n",
            "Time taken for 1 epoch 11.68 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1761\n",
            "Epoch 10 Batch 50 Loss 1.1656\n",
            "Epoch 10 Batch 100 Loss 1.1846\n",
            "Epoch 10 Batch 150 Loss 1.1413\n",
            "\n",
            "Epoch 10 Loss: 1.1617\n",
            "Time taken for 1 epoch 11.85 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perbedaan Onestep dengan CustomTraining**"
      ],
      "metadata": {
        "id": "9JqPEsh8Csfj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. OneStep"
      ],
      "metadata": {
        "id": "UIe4fcwPDl_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pendekatan OneStep melibatkan penggunaan loop pelatihan melalui serangkaian epoch yang telah ditentukan sebelumnya, dan beberapa fitur yang mencakup:\n",
        "  * Penggunaan konstanta EPOCHS untuk menentukan jumlah iterasi pelatihan (epoch).\n",
        "  * Pemanfaatan objek tf.metrics.Mean() untuk menghitung rata-rata loss selama setiap epoch.\n",
        "  * Penggunaan loop bersarang untuk mengiterasi melalui batch data dalam dataset.\n",
        "  * Pada setiap iterasi batch, pemanggilan metode model.train_step([inp, target]) untuk menghitung loss dan perhitungan gradien.\n",
        "  * Pembaruan rata-rata loss menggunakan metode mean.update_state().\n",
        "  * Pencetakan nilai loss setiap 50 batch.\n",
        "  * Penyimpanan checkpoint model setiap 5 epoch.\n",
        "\n",
        "* OneStep mencetak rata-rata loss dan waktu yang diperlukan untuk satu epoch pada akhir setiap epoch.\n"
      ],
      "metadata": {
        "id": "UVaptPO-DpK4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Custom Training"
      ],
      "metadata": {
        "id": "bbpDPTN_EA-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Pendekatan CustomTraining mencakup penggunaan definisi kelas khusus, yaitu kelas CustomTraining, yang mewarisi fungsionalitas dari kelas MyModel.\n",
        "\n",
        "* Dalam kelas CustomTraining, terdapat metode train_step, yang didekorasi dengan @tf.function untuk mengoptimalkan eksekusi menggunakan TensorFlow. Metode ini berperan dalam menghitung loss dan menjalankan pelatihan sebagai berikut:\n",
        "  * Penguraian data masukan menjadi inputs dan labels.\n",
        "  * Penggunaan tape gradien (tf.GradientTape) untuk menghitung gradien dari loss.\n",
        "  * Penggunaan gradien tersebut untuk mengupdate parameter model dengan bantuan optimisasi.\n",
        "  * Pengembalian nilai loss dalam bentuk dictionary setiap kali metode train_step dipanggil."
      ],
      "metadata": {
        "id": "WzcqzhG7ECr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kesimpulan"
      ],
      "metadata": {
        "id": "-zv_uNa8EQhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OneStep bertujuan untuk mengorganisir loop pelatihan secara manual, sedangkan CustomTraining adalah definisi bagaimana model sebenarnya melakukan pelatihan saat dipanggil melalui metode train_step selama loop pelatihan. Biasanya, kedua pendekatan ini digunakan bersama-sama dalam rangkaian pelatihan, dengan loop pelatihan memanggil metode train_step yang telah didefinisikan dalam kelas model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4CR0PpuxESa1"
      }
    }
  ]
}